{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nishant/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.8483e+22, 3.2771e+35, 4.8408e+30],\n",
      "        [6.4486e+22, 1.6278e+25, 9.9839e+11]])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.rand(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9640, 0.1124],\n",
      "        [0.2213, 0.1020]])\n"
     ]
    }
   ],
   "source": [
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(2,2)\n",
    "b = torch.rand(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4055, 1.2606],\n",
      "        [0.7906, 0.8050]])\n"
     ]
    }
   ],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.ones(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = d.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) /tmp/pip-install-0y4qgdsb/opencv-python/opencv/modules/highgui/src/window.cpp:971: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'imshow'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8f7b16e58780>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WEBCAM\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;36m0xFF\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'q'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.7.0) /tmp/pip-install-0y4qgdsb/opencv-python/opencv/modules/highgui/src/window.cpp:971: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'imshow'\n"
     ]
    }
   ],
   "source": [
    "#import cv2 as cv\n",
    "#cap = cv.VideoCapture(0)\n",
    "#cap.set(3,640)\n",
    "#cap.set(4,480)\n",
    "\n",
    "#while True:\n",
    "\n",
    "    #success, img = cap.read()\n",
    "    #cv.imshow(\"WEBCAM\",img)\n",
    "    #if cv.waitKey(1) & 0xFF ==ord('q'):\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/92/87e99adfaf9c847021ecfcd4bf2732c863752cf47c0c9ad349c8de260183/opencv-python-4.7.0.68.tar.gz (91.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 91.1MB 739kB/s eta 0:00:01    59% |███████████████████▏            | 54.5MB 5.7MB/s eta 0:00:07\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.17.0; python_version >= \"3.7\" in ./anaconda3/lib/python3.7/site-packages (from opencv-python) (1.21.6)\n",
      "Building wheels for collected packages: opencv-python\n",
      "  Building wheel for opencv-python (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/nishant/.cache/pip/wheels/61/50/e3/497786c1908ea12c177ef7a527b6c30097a2a5905d6af0a1c2\n",
      "Successfully built opencv-python\n",
      "Installing collected packages: opencv-python\n",
      "  Found existing installation: opencv-python 4.5.3.56\n",
      "    Uninstalling opencv-python-4.5.3.56:\n",
      "      Successfully uninstalled opencv-python-4.5.3.56\n",
      "Successfully installed opencv-python-4.7.0.68\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "img = cv.imread(r'/home/nishant/Pictures/Seattle_Skyline.png')\n",
    "cv.imshow('Space_Needle',img)\n",
    "cv.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.rand(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0094, 0.7933],\n",
      "        [0.8076, 0.9375]])\n"
     ]
    }
   ],
   "source": [
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones_like(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv.imread(r'/home/nishant/Pictures/Seattle_Skyline.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[15 26 34]\n",
      "  [15 26 34]\n",
      "  [15 26 34]\n",
      "  ...\n",
      "  [22 53 78]\n",
      "  [23 54 79]\n",
      "  [23 54 79]]\n",
      "\n",
      " [[14 25 33]\n",
      "  [14 25 33]\n",
      "  [14 25 33]\n",
      "  ...\n",
      "  [22 53 78]\n",
      "  [22 53 78]\n",
      "  [23 54 79]]\n",
      "\n",
      " [[14 25 33]\n",
      "  [14 25 33]\n",
      "  [14 25 33]\n",
      "  ...\n",
      "  [22 53 78]\n",
      "  [22 53 78]\n",
      "  [22 53 78]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0  2  2]\n",
      "  [ 0  2  2]\n",
      "  [ 0  2  2]\n",
      "  ...\n",
      "  [ 2  4  4]\n",
      "  [ 1  3  3]\n",
      "  [ 1  3  3]]\n",
      "\n",
      " [[ 0  2  2]\n",
      "  [ 0  2  2]\n",
      "  [ 0  2  2]\n",
      "  ...\n",
      "  [ 2  4  4]\n",
      "  [ 2  4  4]\n",
      "  [ 1  3  3]]\n",
      "\n",
      " [[ 1  3  3]\n",
      "  [ 0  2  2]\n",
      "  [ 0  2  2]\n",
      "  ...\n",
      "  [ 0  2  2]\n",
      "  [ 0  2  2]\n",
      "  [ 0  2  2]]]\n"
     ]
    }
   ],
   "source": [
    "print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.imshow(\"Image\",image)\n",
    "cv.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1\n",
      "0.8.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "train_data = datasets.FashionMNIST(\n",
    "root=\"data\",train=True,download=True,\n",
    "transform=ToTensor(),target_transform=None)\n",
    "\n",
    "test_data = datasets.FashionMNIST(root = \"data\",train= False,download=True,transform=ToTensor(),\n",
    "                                 target_transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data) , len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n",
       "           0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.0039, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n",
       "           0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n",
       "           0.0157, 0.0000, 0.0000, 0.0118],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n",
       "           0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0471, 0.0392, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n",
       "           0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n",
       "           0.3020, 0.5098, 0.2824, 0.0588],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n",
       "           0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n",
       "           0.5529, 0.3451, 0.6745, 0.2588],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n",
       "           0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n",
       "           0.4824, 0.7686, 0.8980, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n",
       "           0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n",
       "           0.8745, 0.9608, 0.6784, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n",
       "           0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n",
       "           0.8627, 0.9529, 0.7922, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n",
       "           0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n",
       "           0.8863, 0.7725, 0.8196, 0.2039],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n",
       "           0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n",
       "           0.9608, 0.4667, 0.6549, 0.2196],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n",
       "           0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n",
       "           0.8510, 0.8196, 0.3608, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n",
       "           0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n",
       "           0.8549, 1.0000, 0.3020, 0.0000],\n",
       "          [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n",
       "           0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n",
       "           0.8784, 0.9569, 0.6235, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n",
       "           0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n",
       "           0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n",
       "           0.9137, 0.9333, 0.8431, 0.0000],\n",
       "          [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n",
       "           0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n",
       "           0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n",
       "           0.8627, 0.9098, 0.9647, 0.0000],\n",
       "          [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n",
       "           0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n",
       "           0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n",
       "           0.8706, 0.8941, 0.8824, 0.0000],\n",
       "          [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n",
       "           0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n",
       "           0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n",
       "           0.8745, 0.8784, 0.8980, 0.1137],\n",
       "          [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n",
       "           0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n",
       "           0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n",
       "           0.8627, 0.8667, 0.9020, 0.2627],\n",
       "          [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n",
       "           0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n",
       "           0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n",
       "           0.7098, 0.8039, 0.8078, 0.4510],\n",
       "          [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n",
       "           0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n",
       "           0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n",
       "           0.6549, 0.6941, 0.8235, 0.3608],\n",
       "          [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n",
       "           0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n",
       "           0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n",
       "           0.7529, 0.8471, 0.6667, 0.0000],\n",
       "          [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n",
       "           0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n",
       "           0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n",
       "           0.3882, 0.2275, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n",
       "           0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000]]]), 9)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image , label = train_data[0]\n",
    "image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = train_data.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T-shirt/top': 0,\n",
       " 'Trouser': 1,\n",
       " 'Pullover': 2,\n",
       " 'Dress': 3,\n",
       " 'Coat': 4,\n",
       " 'Sandal': 5,\n",
       " 'Shirt': 6,\n",
       " 'Sneaker': 7,\n",
       " 'Bag': 8,\n",
       " 'Ankle boot': 9}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_to_idx = train_data.class_to_idx\n",
    "class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 0, 0,  ..., 3, 0, 5])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 28, 28]), 9)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the shape of Image\n",
    "image.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Shape: torch.Size([1, 28, 28]) --> [color Channels,Height, Width]\n",
      "Image label: Ankle boot\n"
     ]
    }
   ],
   "source": [
    "print(f\"Image Shape: {image.shape} --> [color Channels,Height, Width]\")\n",
    "print(f\"Image label: {class_names[label]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '9')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize Image\n",
    "import matplotlib.pyplot as plt\n",
    "image, label = train_data[0]\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "plt.imshow(image.squeeze())\n",
    "plt.title(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd7ea796860>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image.squeeze(),cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'bool' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-207c2cae605e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gray\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36maxis\u001b[0;34m(*v, **kwargs)\u001b[0m\n\u001b[1;32m   2433\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_dedent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2434\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2435\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36maxis\u001b[0;34m(self, *v, **kwargs)\u001b[0m\n\u001b[1;32m   1712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1713\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1714\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1715\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'v must contain [xmin xmax ymin ymax]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'bool' has no len()"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot More Images\n",
    "\n",
    "torch.manual_seed(42)\n",
    "fig = plt.figure(figsize=(9,9))\n",
    "rows , cols = 4 , 4\n",
    "for i in range(1,rows*cols+1):\n",
    "    random_idx = torch.randint(0,len(train_data), size=[1]).item()\n",
    "    img,label = train_data[random_idx]\n",
    "    fig.add_subplot(rows,cols,i)\n",
    "    plt.imshow(img.squeeze(),cmap=\"gray\")\n",
    "    plt.title(class_names[label])\n",
    "    plt.axis(False);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset FashionMNIST\n",
       "     Number of datapoints: 60000\n",
       "     Root location: data\n",
       "     Split: Train\n",
       "     StandardTransform\n",
       " Transform: ToTensor(), Dataset FashionMNIST\n",
       "     Number of datapoints: 10000\n",
       "     Root location: data\n",
       "     Split: Test\n",
       "     StandardTransform\n",
       " Transform: ToTensor())"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7fd7ea79f978>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7fd7fc7986a0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE = 32\n",
    "train_dataloader = DataLoader(dataset=train_data,\n",
    "                             batch_size=BATCH_SIZE,shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data,batch_size=BATCH_SIZE,shuffle=False)\n",
    "\n",
    "train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders: (<torch.utils.data.dataloader.DataLoader object at 0x7fd7ea79f978>, <torch.utils.data.dataloader.DataLoader object at 0x7fd7fc7986a0>)\n",
      "Length of Train DataLoader:1875 batches of 32...\n",
      "Length of Test DataLoader:313 batches of 32...\n"
     ]
    }
   ],
   "source": [
    "print(f\"DataLoaders: {train_dataloader,test_dataloader}\")\n",
    "print(f\"Length of Train DataLoader:{len(train_dataloader)} batches of {BATCH_SIZE}...\")\n",
    "print(f\"Length of Test DataLoader:{len(test_dataloader)} batches of {BATCH_SIZE}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 28, 28]), torch.Size([32]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out what's inside the training dataloader\n",
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
    "train_features_batch.shape, train_labels_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creation of Flatten Model\n",
    "flatten_model = nn.Flatten()\n",
    "x = train_features_batch[0]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Before flattening: torch.Size([1, 28, 28])\n",
      "Shape After Flattening: torch.Size([1, 784])\n"
     ]
    }
   ],
   "source": [
    "output = flatten_model(x)\n",
    "print(f\"Shape Before flattening: {x.shape}\")\n",
    "print(f\"Shape After Flattening: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class FashionMNIST1vO(nn.Module):\n",
    "    def __init__(self,\n",
    "                input_shape: int,\n",
    "                hidden_units: int,\n",
    "                output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_features=input_shape,out_features=hidden_units),\n",
    "        nn.Linear(in_features=hidden_units,out_features=output_shape)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNIST1vO(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "#Set Model with input parameters\n",
    "model_O = FashionMNIST1vO(\n",
    "input_shape= 28*28, #This is 28 X 28\n",
    "hidden_units= 10, #No of Units in the Hidden Layer\n",
    "output_shape= len(class_names) #For every class\n",
    ").to(\"cpu\")\n",
    "\n",
    "model_O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0315,  0.3171,  0.0531, -0.2525,  0.5959,  0.2112,  0.3233,  0.2694,\n",
       "         -0.1004,  0.0157]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_x = torch.rand(1,1,28,28)\n",
    "model_O(dummy_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer_stack.1.weight',\n",
       "              tensor([[ 0.0273,  0.0296, -0.0084,  ..., -0.0142,  0.0093,  0.0135],\n",
       "                      [-0.0188, -0.0354,  0.0187,  ..., -0.0106, -0.0001,  0.0115],\n",
       "                      [-0.0008,  0.0017,  0.0045,  ..., -0.0127, -0.0188,  0.0059],\n",
       "                      ...,\n",
       "                      [-0.0116,  0.0273, -0.0344,  ...,  0.0176,  0.0283, -0.0011],\n",
       "                      [-0.0230,  0.0257,  0.0291,  ..., -0.0187, -0.0087,  0.0001],\n",
       "                      [ 0.0176, -0.0147,  0.0053,  ..., -0.0336, -0.0221,  0.0205]])),\n",
       "             ('layer_stack.1.bias',\n",
       "              tensor([-0.0093,  0.0283, -0.0033,  0.0255,  0.0017,  0.0037, -0.0302, -0.0123,\n",
       "                       0.0018,  0.0163])),\n",
       "             ('layer_stack.2.weight',\n",
       "              tensor([[ 0.0614, -0.0687,  0.0021,  0.2718,  0.2109,  0.1079, -0.2279, -0.1063,\n",
       "                        0.2019,  0.2847],\n",
       "                      [-0.1495,  0.1344, -0.0740,  0.2006, -0.0475, -0.2514, -0.3130, -0.0118,\n",
       "                        0.0932, -0.1864],\n",
       "                      [ 0.2488,  0.1500,  0.1907,  0.1457, -0.3050, -0.0580,  0.1643,  0.1565,\n",
       "                       -0.2877, -0.1792],\n",
       "                      [ 0.2305, -0.2618,  0.2397, -0.0610,  0.0232,  0.1542,  0.0851, -0.2027,\n",
       "                        0.1030, -0.2715],\n",
       "                      [-0.1596, -0.0555, -0.0633,  0.2302, -0.1726,  0.2654,  0.1473,  0.1029,\n",
       "                        0.2252, -0.2160],\n",
       "                      [-0.2725,  0.0118,  0.1559,  0.1596,  0.0132,  0.3024,  0.1124,  0.1366,\n",
       "                       -0.1533,  0.0965],\n",
       "                      [-0.1184, -0.2555, -0.2057, -0.1909, -0.0477, -0.1324,  0.2905,  0.1307,\n",
       "                       -0.2629,  0.0133],\n",
       "                      [ 0.2727, -0.0127,  0.0513,  0.0863, -0.1043, -0.2047, -0.1185, -0.0825,\n",
       "                        0.2488, -0.2571],\n",
       "                      [ 0.0425, -0.1209, -0.0336, -0.0281, -0.1227,  0.0730,  0.0747, -0.1816,\n",
       "                        0.1943,  0.2853],\n",
       "                      [-0.1310,  0.0645, -0.1171,  0.2168, -0.0245, -0.2820,  0.0736,  0.2621,\n",
       "                        0.0012, -0.0810]])),\n",
       "             ('layer_stack.2.bias',\n",
       "              tensor([-0.0087,  0.1791,  0.2712, -0.0791,  0.1685,  0.1762,  0.2825,  0.2266,\n",
       "                      -0.2612, -0.2613]))])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_O.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading helper_functions.py\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# Download helper functions from Learn PyTorch repo\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "  print(\"helper_functions.py already exists, skipping download...\")\n",
    "else:\n",
    "  print(\"Downloading helper_functions.py\")\n",
    "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "  with open(\"helper_functions.py\", \"wb\") as f:\n",
    "    f.write(request.content)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import accuracy_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up of Loss Function & Optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_O.parameters(),\n",
    "                           lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function helper_functions.accuracy_fn(y_true, y_pred)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer \n",
    "def print_train_time(start: float,\n",
    "                     end: float, \n",
    "                     device: torch.device = None):\n",
    "  \"\"\"Prints difference between start and end time.\"\"\"\n",
    "  total_time = end - start\n",
    "  print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "  return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time on cpu: 0.000 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.825898652896285e-05"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = timer()\n",
    "# some code...\n",
    "end_time = timer()\n",
    "print_train_time(start=start_time, end=end_time, device=\"cpu\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tqdm for progess bar\n",
    "from tqdm.auto import tqdm\n",
    "#set the seed and start the timer\n",
    "torch.manual_seed(42)\n",
    "train_time_start_on_cpu = timer()\n",
    "#set the number of epochs(keeping it small for faster training time)\n",
    "epochs = 3\n",
    "#creating training and testing loop\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n-------\")\n",
    "    ### training\n",
    "    train_loss = 0\n",
    "    # Add a loop to loop through training batches\n",
    "    for batch,(X,y) in enumerate(train_dataloader):\n",
    "        model_O.train()\n",
    "        #forward pass\n",
    "        y_pred = model_O(X)\n",
    "        \n",
    "        #Calculate the loss\n",
    "        \n",
    "        loss = loss_fn(y_pred,y)\n",
    "        train_loss += loss #Accumulate the train loss\n",
    "        \n",
    "        #Optimize the zero grad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Loss Backward\n",
    "        loss.backward()\n",
    "        \n",
    "        #Optimizer Step\n",
    "        optimizer.step()\n",
    "        \n",
    "        #print what's happening\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"Looked at {batch * len(x)}/{len(train_dataloader)} samples.\")\n",
    "        \n",
    "        train_loss /= len(train_dataloader)\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 44)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m44\u001b[0m\n\u001b[0;31m    model_O.eval()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# Import tqdm for progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set the seed and start the timer\n",
    "torch.manual_seed(42)\n",
    "train_time_start_on_cpu = timer() \n",
    "\n",
    "# Set the number of epochs (we'll keep this small for faster training time)\n",
    "epochs = 3\n",
    "\n",
    "# Create training and test loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n------\")\n",
    "  ### Training\n",
    "    train_loss = 0\n",
    "  # Add a loop to loop through the training batches\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        model_O.train()\n",
    "    # 1. Forward pass\n",
    "        y_pred = model_O(X)\n",
    "    \n",
    "    # 2. Calculate loss (per batch)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss # accumulate train loss\n",
    "    \n",
    "    # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    # 4. Loss backward\n",
    "        loss.backward()\n",
    "    \n",
    "    # 5. Optimizer step (update the model's parameters once *per batch*)\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Print out what's happening\n",
    "    if batch % 400 == 0:\n",
    "        print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples.\")\n",
    "  \n",
    "  # Divide total train loss by length of train dataloader\n",
    "train_loss /= len(train_dataloader)\n",
    "\n",
    "  ### Testing\n",
    "      test_loss, test_acc = 0, 0\n",
    "    model_O.eval()\n",
    "    with torch.inference_mode():\n",
    "        \n",
    "        for X_test, y_test in test_dataloader:\n",
    "            test_pred = model_O(X_test)\n",
    "      # 2. Calculate loss (accumulatively)\n",
    "            test_loss += loss_fn(test_pred, y_test)\n",
    "\n",
    "      # 3. Calculate accuracy\n",
    "            test_acc += accuracy_fn(y_true=y_test, y_pred=test_pred.argmax(dim=1))\n",
    "\n",
    "    # Calculate the test loss average per batch\n",
    "            test_loss /= len(test_dataloader)\n",
    "\n",
    "    # Calculate the test acc average per batch\n",
    "            test_acc /= len(test_dataloader)\n",
    "\n",
    "  # Print out what's happening\n",
    "            print(f\"\\nTrain loss: {train_loss:.4f} | Test loss: {test_loss:.4f}, Test acc: {test_acc:.4f}\")\n",
    "\n",
    "# Calculate training time\n",
    "train_time_end_on_cpu = timer()\n",
    "total_train_time_model_O = print_train_time(start=train_time_start_on_cpu,\n",
    "                                            end=train_time_end_on_cpu,\n",
    "                                            device=str(next(model_O.parameters()).device))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1163897530ab4ffd97c2ee1070c3831d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "------\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'inference_mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-ac2a50cc2f18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mmodel_O\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_O\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'inference_mode'"
     ]
    }
   ],
   "source": [
    "# Import tqdm for progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set the seed and start the timer\n",
    "torch.manual_seed(42)\n",
    "train_time_start_on_cpu = timer() \n",
    "\n",
    "# Set the number of epochs (we'll keep this small for faster training time)\n",
    "epochs = 3\n",
    "\n",
    "# Create training and test loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n------\")\n",
    "  ### Training\n",
    "    train_loss = 0\n",
    "  # Add a loop to loop through the training batches\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        model_O.train()\n",
    "        y_pred = model_O(X)\n",
    "        \n",
    "        loss = loss_fn(y_pred, y)\n",
    "        \n",
    "        train_loss += loss # accumulate train loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    # Print out what's happening\n",
    "    if batch % 400 == 0:\n",
    "        print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples.\")\n",
    "  \n",
    "  # Divide total train loss by length of train dataloader\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "  ### Testing\n",
    "    test_loss, test_acc = 0, 0\n",
    "    model_O.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X_test, y_test in test_dataloader:\n",
    "            test_pred = model_O(X_test)\n",
    "            \n",
    "            test_loss += loss_fn(test_pred, y_test)\n",
    "            \n",
    "            test_acc += accuracy_fn(y_true=y_test, y_pred=test_pred.argmax(dim=1))\n",
    "            \n",
    "            test_loss /= len(test_dataloader)\n",
    "            \n",
    "            test_acc /= len(test_dataloader)\n",
    "            \n",
    "            print(f\"\\nTrain loss: {train_loss:.4f} | Test loss: {test_loss:.4f}, Test acc: {test_acc:.4f}\")\n",
    "\n",
    "# Calculate training time\n",
    "train_time_end_on_cpu = timer()\n",
    "total_train_time_model_O = print_train_time(start=train_time_start_on_cpu,\n",
    "                                            end=train_time_end_on_cpu,\n",
    "                                            device=str(next(model_O.parameters()).device))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./anaconda3/lib/python3.7/site-packages (1.7.1)\n",
      "Collecting torch\n",
      "  Downloading torch-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (887.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0mB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:04\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m \u001b[32m543.9/557.1 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\u001b[31mERROR: Could not install packages due to an OSError: [Errno 28] No space left on device\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[2K     \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m \u001b[32m544.2/557.1 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
